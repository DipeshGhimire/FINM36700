{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd8d8a8",
   "metadata": {},
   "source": [
    "## Import necessary packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad8e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeac2c2",
   "metadata": {},
   "source": [
    "### Settings and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b19543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"spx_data_weekly.xlsx\"\n",
    "\n",
    "WEEKS_PER_YEAR = 52\n",
    "MIN_YEARS_CONTINUOUS = 5\n",
    "MIN_WEEKS_CONTINUOUS = MIN_YEARS_CONTINUOUS * WEEKS_PER_YEAR\n",
    "\n",
    "LONG_SHORT_QUANTILE = 0.20\n",
    "POSITION_WEIGHT = 0.01\n",
    "\n",
    "VAR_QUANTILE = 0.05\n",
    "ROLLING_WINDOW_WEEKS = 5 * WEEKS_PER_YEAR\n",
    "\n",
    "MAG7_TICKERS = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'NVDA', 'META', 'TSLA']\n",
    "\n",
    "LASSO_TARGET_TICKER = 'XLK'\n",
    "LASSO_CV_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44806fef",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30bdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading\n",
    "def load_panel_sheet(path: str, sheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Load sheet with 2-row headers into MultiIndex DataFrame\"\"\"\n",
    "    df = pd.read_excel(path, sheet_name=sheet_name, header=[0, 1], index_col=0)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.columns.names = ['ticker', 'field']\n",
    "    return df.sort_index()\n",
    "\n",
    "def extract_field(panel: pd.DataFrame, field: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract (date x ticker) DataFrame for a specific field\"\"\"\n",
    "    return panel.xs(field, axis=1, level='field').sort_index(axis=1)\n",
    "\n",
    "def compute_total_returns(prices: pd.DataFrame, dividend_yields: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute weekly total returns including dividend yield carry.\n",
    "    r_t = (P_t / P_{t-1} - 1) + (dvd_yld_{t-1} / 100) / 52\n",
    "    \"\"\"\n",
    "    price_returns = prices.pct_change()\n",
    "    dividend_carry = (dividend_yields.shift(1) / 100.0) / WEEKS_PER_YEAR\n",
    "    return price_returns + dividend_carry\n",
    "\n",
    "## Data Filtering\n",
    "def longest_continuous_run(series: pd.Series) -> int:\n",
    "    \"\"\"Find longest consecutive run of non-null values\"\"\"\n",
    "    is_valid = series.notna().to_numpy()\n",
    "    if not is_valid.any():\n",
    "        return 0\n",
    "    \n",
    "    padded = np.concatenate([[False], is_valid, [False]])\n",
    "    boundaries = np.where(padded[1:] != padded[:-1])[0]\n",
    "    run_lengths = boundaries[1::2] - boundaries[::2]\n",
    "    return int(run_lengths.max()) if len(run_lengths) > 0 else 0\n",
    "\n",
    "def filter_tickers_by_history(prices: pd.DataFrame, min_weeks: int) -> list:\n",
    "    \"\"\"Keep tickers with at least min_weeks of continuous price data\"\"\"\n",
    "    valid_tickers = []\n",
    "    for ticker in prices.columns:\n",
    "        if longest_continuous_run(prices[ticker]) >= min_weeks:\n",
    "            valid_tickers.append(ticker)\n",
    "    return valid_tickers\n",
    "\n",
    "## Portfolio Construction\n",
    "def build_portfolio_weights(signal: pd.DataFrame, quantile: float, \n",
    "                           weight: float, long_short: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build portfolio weights based on signal ranking.\n",
    "    Long top quantile, optionally short bottom quantile.\n",
    "    \"\"\"\n",
    "    weights = pd.DataFrame(0.0, index=signal.index, columns=signal.columns)\n",
    "    \n",
    "    for date in signal.index:\n",
    "        cross_section = signal.loc[date].dropna()\n",
    "        n_stocks = len(cross_section)\n",
    "        \n",
    "        if n_stocks == 0:\n",
    "            continue\n",
    "        \n",
    "        n_select = int(np.floor(quantile * n_stocks))\n",
    "        if n_select < 1:\n",
    "            continue\n",
    "        \n",
    "        ranked = cross_section.sort_values(ascending=False)\n",
    "        long_names = ranked.head(n_select).index\n",
    "        weights.loc[date, long_names] = weight\n",
    "        \n",
    "        if long_short:\n",
    "            short_names = ranked.tail(n_select).index\n",
    "            weights.loc[date, short_names] = -weight\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def backtest_portfolio(weights: pd.DataFrame, returns: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate portfolio returns using weights from t-1 and returns at t.\n",
    "    R_t = sum_i w_{t-1,i} * r_{t,i}\n",
    "    \"\"\"\n",
    "    common_tickers = weights.columns.intersection(returns.columns)\n",
    "    aligned_weights = weights[common_tickers]\n",
    "    aligned_returns = returns[common_tickers]\n",
    "    \n",
    "    portfolio_returns = (aligned_weights.shift(1) * aligned_returns).sum(axis=1)\n",
    "    return portfolio_returns.dropna()\n",
    "\n",
    "## Performance Metrics\n",
    "def calculate_performance_metrics(returns: pd.Series, risk_free: pd.Series = None) -> dict:\n",
    "    \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "    returns_clean = returns.dropna()\n",
    "    \n",
    "    if risk_free is not None:\n",
    "        rf_aligned = risk_free.reindex(returns_clean.index)\n",
    "        excess_returns = (returns_clean - rf_aligned).dropna()\n",
    "        returns_clean = returns_clean.reindex(excess_returns.index)\n",
    "    else:\n",
    "        excess_returns = returns_clean\n",
    "    \n",
    "    # Annualized metrics\n",
    "    ann_mean = excess_returns.mean() * WEEKS_PER_YEAR\n",
    "    ann_vol = excess_returns.std() * np.sqrt(WEEKS_PER_YEAR)\n",
    "    ann_sharpe = ann_mean / ann_vol if ann_vol > 0 else np.nan\n",
    "    \n",
    "    # Tail risk metrics\n",
    "    skewness = excess_returns.skew()\n",
    "    var_5 = np.quantile(excess_returns, VAR_QUANTILE)\n",
    "    cvar_5 = excess_returns[excess_returns <= var_5].mean()\n",
    "    \n",
    "    # Max drawdown\n",
    "    cumulative_wealth = (1 + returns_clean).cumprod()\n",
    "    running_max = cumulative_wealth.cummax()\n",
    "    drawdowns = (cumulative_wealth - running_max) / running_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "    \n",
    "    return {\n",
    "        'ann_mean': ann_mean,\n",
    "        'ann_vol': ann_vol,\n",
    "        'ann_sharpe': ann_sharpe,\n",
    "        'skewness': skewness,\n",
    "        'VaR_5%': var_5,\n",
    "        'CVaR_5%': cvar_5,\n",
    "        'max_drawdown': max_drawdown\n",
    "    }\n",
    "\n",
    "def create_performance_table(strategies: dict, risk_free: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Create performance comparison table for multiple strategies\"\"\"\n",
    "    results = []\n",
    "    for name, returns in strategies.items():\n",
    "        metrics = calculate_performance_metrics(returns, risk_free)\n",
    "        metrics['strategy'] = name\n",
    "        results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(results).set_index('strategy')\n",
    "\n",
    "## Factor Regressions\n",
    "def linear_factor_decomposition_univariate(y: pd.Series, x: pd.Series) -> dict:\n",
    "    \"\"\"Univariate LFD: y = alpha + beta*x + epsilon\"\"\"\n",
    "    data = pd.concat({'y': y, 'x': x}, axis=1).dropna()\n",
    "    X = sm.add_constant(data['x'])\n",
    "    model = sm.OLS(data['y'], X).fit()\n",
    "    \n",
    "    return {\n",
    "        'alpha_ann': model.params['const'] * WEEKS_PER_YEAR,\n",
    "        'beta': model.params['x'],\n",
    "        'r2': model.rsquared\n",
    "    }\n",
    "\n",
    "def linear_factor_decomposition_multivariate(y: pd.Series, X: pd.DataFrame) -> dict:\n",
    "    \"\"\"Multivariate LFD: y = alpha + B'X + epsilon\"\"\"\n",
    "    data = pd.concat([y.rename('y'), X], axis=1).dropna()\n",
    "    y_vec = data['y']\n",
    "    X_mat = sm.add_constant(data.drop(columns=['y']))\n",
    "    model = sm.OLS(y_vec, X_mat).fit()\n",
    "    \n",
    "    alpha_ann = model.params['const'] * WEEKS_PER_YEAR\n",
    "    betas = model.params.drop('const')\n",
    "    \n",
    "    return {\n",
    "        'alpha_ann': alpha_ann,\n",
    "        'betas': betas,\n",
    "        'r2': model.rsquared\n",
    "    }\n",
    "\n",
    "def calculate_correlation_matrix(series_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"Calculate correlation matrix for multiple return series\"\"\"\n",
    "    df = pd.concat(series_dict, axis=1).dropna()\n",
    "    df.columns = list(series_dict.keys())\n",
    "    return df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8952882",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f29b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tickers after 5-year filter: 485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "spx_names = pd.read_excel(DATA_PATH, sheet_name='spx names', index_col=0)\n",
    "spx_panel = load_panel_sheet(DATA_PATH, 'spx data')\n",
    "\n",
    "# Extract fields\n",
    "prices = extract_field(spx_panel, 'PX_LAST')\n",
    "dividend_yields = extract_field(spx_panel, 'EQY_DVD_YLD_IND')\n",
    "pe_ratios = extract_field(spx_panel, 'pe ratio')\n",
    "\n",
    "# Filter tickers with >= 5 years continuous data\n",
    "valid_tickers = filter_tickers_by_history(prices, MIN_WEEKS_CONTINUOUS)\n",
    "prices = prices[valid_tickers]\n",
    "dividend_yields = dividend_yields[valid_tickers]\n",
    "pe_ratios = pe_ratios[valid_tickers]\n",
    "\n",
    "print(f\"Number of tickers after 5-year filter: {len(valid_tickers)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6815123",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e201d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most frequent HIGHEST dividend yield tickers (raw):\n",
      "MO      127\n",
      "DD       97\n",
      "T        92\n",
      "KDP      71\n",
      "OKE      27\n",
      "CTRA     27\n",
      "TRGP     19\n",
      "DOW      15\n",
      "FANG     13\n",
      "WMB      12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most frequent LOWEST dividend yield tickers (raw):\n",
      "COO     287\n",
      "CI      154\n",
      "NVDA     81\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most frequent HIGHEST 1-year avg dividend yield tickers:\n",
      "MO      128\n",
      "T       109\n",
      "DD       88\n",
      "KDP      60\n",
      "OKE      50\n",
      "TRGP     22\n",
      "CTRA     14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most frequent LOWEST 1-year avg dividend yield tickers:\n",
      "COO     244\n",
      "CI      146\n",
      "NVDA     81\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find highest/lowest yield stocks for each date\n",
    "highest_yield_ticker = dividend_yields.idxmax(axis=1)\n",
    "lowest_yield_ticker = dividend_yields.idxmin(axis=1)\n",
    "\n",
    "print(\"\\nMost frequent HIGHEST dividend yield tickers (raw):\")\n",
    "print(highest_yield_ticker.value_counts().head(10))\n",
    "\n",
    "print(\"\\nMost frequent LOWEST dividend yield tickers (raw):\")\n",
    "print(lowest_yield_ticker.value_counts().head(10))\n",
    "\n",
    "# Calculate 1-year rolling average\n",
    "rolling_avg_yield = dividend_yields.rolling(window=52, min_periods=52).mean()\n",
    "highest_yield_1y = rolling_avg_yield.idxmax(axis=1)\n",
    "lowest_yield_1y = rolling_avg_yield.idxmin(axis=1)\n",
    "\n",
    "print(\"\\nMost frequent HIGHEST 1-year avg dividend yield tickers:\")\n",
    "print(highest_yield_1y.value_counts().head(10))\n",
    "\n",
    "print(\"\\nMost frequent LOWEST 1-year avg dividend yield tickers:\")\n",
    "print(lowest_yield_1y.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cd8bebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Price Volatility: 0.04345133122816815\n",
      "Average Implied Dividend Volatility: 0.04072029755561552\n"
     ]
    }
   ],
   "source": [
    "# Analyze whether driven by D or P\n",
    "implied_dividend = (dividend_yields / 100.0) * prices\n",
    "print(f'Average Price Volatility: {prices.pct_change().std().mean()}')\n",
    "print(f'Average Implied Dividend Volatility: {implied_dividend.pct_change().std().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f198864",
   "metadata": {},
   "source": [
    "#### It suggests that yeilds changes are driven more by price movements rather than dividend changes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b063e72",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "582c6047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2015-07-03    0.000000\n",
       "2015-07-10    0.005680\n",
       "2015-07-17    0.005415\n",
       "2015-07-24   -0.016078\n",
       "2015-07-31    0.017678\n",
       "                ...   \n",
       "2025-05-30    0.012107\n",
       "2025-06-06    0.006138\n",
       "2025-06-13    0.001250\n",
       "2025-06-20   -0.001566\n",
       "2025-06-27    0.009654\n",
       "Name: LO, Length: 522, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate total returns\n",
    "stock_returns = compute_total_returns(prices, dividend_yields)\n",
    "\n",
    "# Build long-only portfolio weights (top 20% by dividend yield)\n",
    "weights_long_only = build_portfolio_weights(\n",
    "    signal=dividend_yields,\n",
    "    quantile=LONG_SHORT_QUANTILE,\n",
    "    weight=POSITION_WEIGHT,\n",
    "    long_short=False\n",
    ")\n",
    "\n",
    "# Backtest\n",
    "returns_long_only = backtest_portfolio(weights_long_only, stock_returns)\n",
    "returns_long_only.name = 'LO'\n",
    "\n",
    "returns_long_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f68223",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d47345a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2015-07-03    0.000000\n",
       "2015-07-10    0.005320\n",
       "2015-07-17   -0.000810\n",
       "2015-07-24   -0.005440\n",
       "2015-07-31    0.006166\n",
       "                ...   \n",
       "2025-05-30    0.002333\n",
       "2025-06-06   -0.006279\n",
       "2025-06-13    0.013152\n",
       "2025-06-20   -0.003044\n",
       "2025-06-27   -0.015634\n",
       "Name: LS, Length: 522, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build long-short portfolio weights (long top 20%, short bottom 20%)\n",
    "weights_long_short = build_portfolio_weights(\n",
    "    signal=dividend_yields,\n",
    "    quantile=LONG_SHORT_QUANTILE,\n",
    "    weight=POSITION_WEIGHT,\n",
    "    long_short=True\n",
    ")\n",
    "\n",
    "# Backtest\n",
    "returns_long_short = backtest_portfolio(weights_long_short, stock_returns)\n",
    "returns_long_short.name = 'LS'\n",
    "\n",
    "returns_long_short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011767f",
   "metadata": {},
   "source": [
    "### 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c03d5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Metrics (annualized, Sharpe uses excess returns over SHV):\n",
      "          ann_mean  ann_vol  ann_sharpe  skewness  VaR_5%  CVaR_5%  \\\n",
      "strategy                                                             \n",
      "LO          0.1371   0.1630      0.8407    0.2972 -0.0277  -0.0489   \n",
      "LS          0.0148   0.0955      0.1548    1.1170 -0.0185  -0.0271   \n",
      "SPY         0.1222   0.1733      0.7052   -0.6057 -0.0345  -0.0571   \n",
      "\n",
      "          max_drawdown  \n",
      "strategy                \n",
      "LO             -0.3542  \n",
      "LS             -0.1349  \n",
      "SPY            -0.3183  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load benchmark data\n",
    "additional_panel = load_panel_sheet(DATA_PATH, 'additional data')\n",
    "additional_prices = extract_field(additional_panel, 'PX_LAST')\n",
    "\n",
    "spy_price = additional_prices['SPY']\n",
    "shv_price = additional_prices['SHV']\n",
    "\n",
    "spy_returns = spy_price.pct_change()\n",
    "spy_returns.name = 'SPY'\n",
    "\n",
    "risk_free_returns = shv_price.pct_change()\n",
    "risk_free_returns.name = 'RF'\n",
    "\n",
    "# Create performance table\n",
    "performance_table = create_performance_table(\n",
    "    {'LO': returns_long_only, 'LS': returns_long_short, 'SPY': spy_returns},\n",
    "    risk_free_returns\n",
    ")\n",
    "\n",
    "print(\"\\nPerformance Metrics (annualized, Sharpe uses excess returns over SHV):\")\n",
    "print(performance_table.round(4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be82da",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8632c58",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29b5ebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Factor Decomposition vs SPY:\n",
      "    alpha_ann    beta      r2\n",
      "LO     0.0487  0.7595  0.6529\n",
      "LS     0.0374 -0.0284  0.0027\n",
      "\n",
      "Correlation Matrix:\n",
      "         LO      LS     SPY\n",
      "LO   1.0000  0.5037  0.8080\n",
      "LS   0.5037  1.0000 -0.0517\n",
      "SPY  0.8080 -0.0517  1.0000\n"
     ]
    }
   ],
   "source": [
    "# LFD vs SPY\n",
    "lfd_lo_spy = linear_factor_decomposition_univariate(returns_long_only, spy_returns)\n",
    "lfd_ls_spy = linear_factor_decomposition_univariate(returns_long_short, spy_returns)\n",
    "\n",
    "market_exposure_df = pd.DataFrame({\n",
    "    'LO': lfd_lo_spy,\n",
    "    'LS': lfd_ls_spy\n",
    "}).T\n",
    "\n",
    "print(\"\\nLinear Factor Decomposition vs SPY:\")\n",
    "print(market_exposure_df.round(4))\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = calculate_correlation_matrix({\n",
    "    'LO': returns_long_only,\n",
    "    'LS': returns_long_short,\n",
    "    'SPY': spy_returns\n",
    "})\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f8e0b",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98a39b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Long-Only Sector LFD:\n",
      "  Alpha (annualized): 0.0683\n",
      "  R-squared: 0.9311\n",
      "  Top 3 sector betas:\n",
      "    XLRE: 0.2540\n",
      "    XLF: 0.2437\n",
      "    XLU: 0.1267\n",
      "\n",
      "Long-Short Sector LFD:\n",
      "  Alpha (annualized): 0.0588\n",
      "  R-squared: 0.6437\n",
      "  Top 3 sector betas:\n",
      "    XLK: -0.2478\n",
      "    XLV: -0.2406\n",
      "    XLRE: 0.2158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load sector data\n",
    "sector_panel = load_panel_sheet(DATA_PATH, 'sector data')\n",
    "sector_prices = extract_field(sector_panel, 'PX_LAST')\n",
    "\n",
    "# Calculate sector returns (exclude SPY and SHV)\n",
    "sector_returns = sector_prices.pct_change()\n",
    "for exclude in ['SPY', 'SHV']:\n",
    "    if exclude in sector_returns.columns:\n",
    "        sector_returns = sector_returns.drop(columns=exclude)\n",
    "\n",
    "# Align with strategy returns\n",
    "sector_returns = sector_returns.loc[returns_long_only.index.intersection(sector_returns.index)]\n",
    "\n",
    "# Multivariate LFD\n",
    "lfd_lo_sectors = linear_factor_decomposition_multivariate(returns_long_only, sector_returns)\n",
    "lfd_ls_sectors = linear_factor_decomposition_multivariate(returns_long_short, sector_returns)\n",
    "\n",
    "print(f\"\\nLong-Only Sector LFD:\")\n",
    "print(f\"  Alpha (annualized): {lfd_lo_sectors['alpha_ann']:.4f}\")\n",
    "print(f\"  R-squared: {lfd_lo_sectors['r2']:.4f}\")\n",
    "print(f\"  Top 3 sector betas:\")\n",
    "for sector in lfd_lo_sectors['betas'].abs().nlargest(3).index:\n",
    "    print(f\"    {sector}: {lfd_lo_sectors['betas'][sector]:.4f}\")\n",
    "\n",
    "print(f\"\\nLong-Short Sector LFD:\")\n",
    "print(f\"  Alpha (annualized): {lfd_ls_sectors['alpha_ann']:.4f}\")\n",
    "print(f\"  R-squared: {lfd_ls_sectors['r2']:.4f}\")\n",
    "print(f\"  Top 3 sector betas:\")\n",
    "for sector in lfd_ls_sectors['betas'].abs().nlargest(3).index:\n",
    "    print(f\"    {sector}: {lfd_ls_sectors['betas'][sector]:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b630c8",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b61ee1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Largest sector exposures (|beta_i * sigma_i|) - Long-Only:\n",
      "XLRE    0.0076\n",
      "XLF     0.0076\n",
      "XLE     0.0050\n",
      "XLU     0.0034\n",
      "XLK     0.0030\n",
      "dtype: float64\n",
      "\n",
      "Largest sector exposures (|beta_i * sigma_i|) - Long-Short:\n",
      "XLK     0.0075\n",
      "XLRE    0.0064\n",
      "XLF     0.0064\n",
      "XLV     0.0056\n",
      "XLE     0.0045\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate beta * sigma for each sector\n",
    "sector_volatility = sector_returns.std()\n",
    "exposure_lo = lfd_lo_sectors['betas'] * sector_volatility\n",
    "exposure_ls = lfd_ls_sectors['betas'] * sector_volatility\n",
    "\n",
    "print(\"\\nLargest sector exposures (|beta_i * sigma_i|) - Long-Only:\")\n",
    "print(exposure_lo.abs().nlargest(5).round(4))\n",
    "\n",
    "print(\"\\nLargest sector exposures (|beta_i * sigma_i|) - Long-Short:\")\n",
    "print(exposure_ls.abs().nlargest(5).round(4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b06e7",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b281632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Long-Only Two-Factor LFD (SPY & MAG):\n",
      "  Alpha (annualized): 0.1246\n",
      "  Beta (SPY): 1.1833\n",
      "  Beta (MAG): -0.3349\n",
      "  R-squared: 0.7656\n",
      "\n",
      "Long-Short Two-Factor LFD (SPY & MAG):\n",
      "  Alpha (annualized): 0.1016\n",
      "  Beta (SPY): 0.3300\n",
      "  Beta (MAG): -0.2833\n",
      "  R-squared: 0.2381\n"
     ]
    }
   ],
   "source": [
    "# Build MAG7 portfolio\n",
    "available_mag7 = [t for t in MAG7_TICKERS if t in stock_returns.columns]\n",
    "if len(available_mag7) < len(MAG7_TICKERS):\n",
    "    missing = set(MAG7_TICKERS) - set(available_mag7)\n",
    "    print(f\"\\nWarning: Missing MAG7 tickers: {missing}\")\n",
    "\n",
    "mag7_returns = stock_returns[available_mag7].mean(axis=1)\n",
    "mag7_returns.name = 'MAG'\n",
    "\n",
    "# Two-factor model (SPY and MAG7)\n",
    "two_factors = pd.concat([spy_returns, mag7_returns], axis=1).dropna()\n",
    "two_factors.columns = ['SPY', 'MAG']\n",
    "\n",
    "lfd_lo_spy_mag = linear_factor_decomposition_multivariate(returns_long_only, two_factors)\n",
    "lfd_ls_spy_mag = linear_factor_decomposition_multivariate(returns_long_short, two_factors)\n",
    "\n",
    "print(\"\\nLong-Only Two-Factor LFD (SPY & MAG):\")\n",
    "print(f\"  Alpha (annualized): {lfd_lo_spy_mag['alpha_ann']:.4f}\")\n",
    "print(f\"  Beta (SPY): {lfd_lo_spy_mag['betas']['SPY']:.4f}\")\n",
    "print(f\"  Beta (MAG): {lfd_lo_spy_mag['betas']['MAG']:.4f}\")\n",
    "print(f\"  R-squared: {lfd_lo_spy_mag['r2']:.4f}\")\n",
    "\n",
    "print(\"\\nLong-Short Two-Factor LFD (SPY & MAG):\")\n",
    "print(f\"  Alpha (annualized): {lfd_ls_spy_mag['alpha_ann']:.4f}\")\n",
    "print(f\"  Beta (SPY): {lfd_ls_spy_mag['betas']['SPY']:.4f}\")\n",
    "print(f\"  Beta (MAG): {lfd_ls_spy_mag['betas']['MAG']:.4f}\")\n",
    "print(f\"  R-squared: {lfd_ls_spy_mag['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9587d8d",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce00766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3.1 - Performance of Sector-Hedged Strategies:\n",
      "           ann_mean  ann_vol  ann_sharpe  skewness  VaR_5%  CVaR_5%  \\\n",
      "strategy                                                              \n",
      "LO_HEDGED    0.0959   0.1756      0.5464    0.5096 -0.0265  -0.0545   \n",
      "LS_HEDGED    0.0097   0.0995      0.0971    1.4811 -0.0176  -0.0267   \n",
      "\n",
      "           max_drawdown  \n",
      "strategy                 \n",
      "LO_HEDGED       -0.3542  \n",
      "LS_HEDGED       -0.1349  \n",
      "\n",
      "3.2 - Sector LFD of Hedged Strategies:\n",
      "\n",
      "LO_HEDGED:\n",
      "  Alpha (annualized): 0.0414\n",
      "  R-squared: 0.7749\n",
      "\n",
      "LS_HEDGED:\n",
      "  Alpha (annualized): 0.0379\n",
      "  R-squared: 0.3804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dynamic_sector_hedge(strategy_returns: pd.Series, factors: pd.DataFrame, \n",
    "                        window: int) -> pd.Series:\n",
    "    \"\"\"Apply rolling sector hedge to strategy returns\"\"\"\n",
    "    data = pd.concat([strategy_returns.rename('y'), factors], axis=1).dropna()\n",
    "    y = data['y']\n",
    "    X = data.drop(columns=['y'])\n",
    "    \n",
    "    # Store rolling betas\n",
    "    betas = pd.DataFrame(index=data.index, columns=X.columns, dtype=float)\n",
    "    \n",
    "    # Rolling regression\n",
    "    for i in range(window - 1, len(data)):\n",
    "        window_dates = data.index[i - window + 1:i + 1]\n",
    "        y_window = y.loc[window_dates]\n",
    "        X_window = X.loc[window_dates]\n",
    "        X_with_const = sm.add_constant(X_window)\n",
    "        model = sm.OLS(y_window, X_with_const).fit()\n",
    "        betas.loc[data.index[i], :] = model.params.drop('const')\n",
    "    \n",
    "    # Apply hedge (use t-1 betas for time t)\n",
    "    hedge = (betas.shift(1) * X).sum(axis=1)\n",
    "    hedged = (y - hedge).dropna()\n",
    "    return hedged\n",
    "\n",
    "# Apply dynamic hedging\n",
    "returns_lo_hedged = dynamic_sector_hedge(returns_long_only, sector_returns, ROLLING_WINDOW_WEEKS)\n",
    "returns_ls_hedged = dynamic_sector_hedge(returns_long_short, sector_returns, ROLLING_WINDOW_WEEKS)\n",
    "\n",
    "# Section 3.1 - Hedged performance\n",
    "print(\"\\n3.1 - Performance of Sector-Hedged Strategies:\")\n",
    "hedged_performance = create_performance_table(\n",
    "    {'LO_HEDGED': returns_lo_hedged, 'LS_HEDGED': returns_ls_hedged},\n",
    "    risk_free_returns\n",
    ")\n",
    "print(hedged_performance.round(4))\n",
    "\n",
    "# Section 3.2 - Sector LFD of hedged strategies\n",
    "print(\"\\n3.2 - Sector LFD of Hedged Strategies:\")\n",
    "lfd_lo_hedged = linear_factor_decomposition_multivariate(returns_lo_hedged, sector_returns)\n",
    "lfd_ls_hedged = linear_factor_decomposition_multivariate(returns_ls_hedged, sector_returns)\n",
    "\n",
    "print(f\"\\nLO_HEDGED:\")\n",
    "print(f\"  Alpha (annualized): {lfd_lo_hedged['alpha_ann']:.4f}\")\n",
    "print(f\"  R-squared: {lfd_lo_hedged['r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nLS_HEDGED:\")\n",
    "print(f\"  Alpha (annualized): {lfd_ls_hedged['alpha_ann']:.4f}\")\n",
    "print(f\"  R-squared: {lfd_ls_hedged['r2']:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f925352",
   "metadata": {},
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "755c12f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.1 - Forecast vs Realized Returns:\n",
      "  Out-of-sample R²: -0.0064\n",
      "  Correlation: 0.0065\n",
      "\n",
      "4.2 - Forecast vs SPY-Hedged Residuals:\n",
      "  Out-of-sample R²: 0.0005\n",
      "  Correlation: 0.0259\n",
      "\n",
      "4.3 - Forecast vs Sector-Hedged Residuals:\n",
      "  Out-of-sample R²: -0.0004\n",
      "  Correlation: 0.0084\n"
     ]
    }
   ],
   "source": [
    "def build_forecasts(signal: pd.DataFrame, quantile: float, \n",
    "                    long_fcst: float, short_fcst: float) -> pd.DataFrame:\n",
    "    \"\"\"Build fixed forecasts based on signal ranking\"\"\"\n",
    "    forecasts = pd.DataFrame(0.0, index=signal.index, columns=signal.columns)\n",
    "    \n",
    "    for date in signal.index:\n",
    "        cross_section = signal.loc[date].dropna()\n",
    "        n_stocks = len(cross_section)\n",
    "        if n_stocks == 0:\n",
    "            continue\n",
    "        \n",
    "        n_select = int(np.floor(quantile * n_stocks))\n",
    "        if n_select < 1:\n",
    "            continue\n",
    "        \n",
    "        ranked = cross_section.sort_values(ascending=False)\n",
    "        forecasts.loc[date, ranked.head(n_select).index] = long_fcst\n",
    "        forecasts.loc[date, ranked.tail(n_select).index] = short_fcst\n",
    "    \n",
    "    return forecasts\n",
    "\n",
    "def evaluate_forecasts(forecasts: pd.DataFrame, realized: pd.DataFrame) -> dict:\n",
    "    \"\"\"Evaluate forecast quality using pooled R2 and correlation\"\"\"\n",
    "    common_idx = forecasts.index.intersection(realized.index)\n",
    "    common_cols = forecasts.columns.intersection(realized.columns)\n",
    "    \n",
    "    fcst = forecasts.loc[common_idx, common_cols].values\n",
    "    real = realized.loc[common_idx, common_cols].values\n",
    "    \n",
    "    mask = np.isfinite(fcst) & np.isfinite(real)\n",
    "    fcst_vec = fcst[mask]\n",
    "    real_vec = real[mask]\n",
    "    \n",
    "    if len(real_vec) == 0:\n",
    "        return {'oos_r2': np.nan, 'correlation': np.nan}\n",
    "    \n",
    "    mse = np.mean((real_vec - fcst_vec) ** 2)\n",
    "    var = np.mean((real_vec - real_vec.mean()) ** 2)\n",
    "    oos_r2 = 1.0 - (mse / var) if var > 0 else np.nan\n",
    "    \n",
    "    correlation = np.corrcoef(fcst_vec, real_vec)[0, 1] if fcst_vec.std() > 0 else np.nan\n",
    "    \n",
    "    return {'oos_r2': oos_r2, 'correlation': correlation}\n",
    "\n",
    "# Build forecasts (+0.1% for longs, -0.1% for shorts)\n",
    "forecasts = build_forecasts(\n",
    "    signal=dividend_yields,\n",
    "    quantile=LONG_SHORT_QUANTILE,\n",
    "    long_fcst=0.001,\n",
    "    short_fcst=-0.001\n",
    ")\n",
    "\n",
    "# Section 4.1 - Forecasts vs realized returns\n",
    "realized_forward = stock_returns.shift(-1)  # Next period realized returns\n",
    "eval_41 = evaluate_forecasts(forecasts, realized_forward)\n",
    "\n",
    "print(\"\\n4.1 - Forecast vs Realized Returns:\")\n",
    "print(f\"  Out-of-sample R²: {eval_41['oos_r2']:.4f}\")\n",
    "print(f\"  Correlation: {eval_41['correlation']:.4f}\")\n",
    "\n",
    "# Section 4.2 - Forecasts vs SPY-hedged residuals\n",
    "def calculate_rolling_residuals_single_factor(returns: pd.DataFrame, \n",
    "                                              factor: pd.Series, \n",
    "                                              window: int) -> pd.DataFrame:\n",
    "    \"\"\"Calculate rolling residuals for each stock vs single factor\"\"\"\n",
    "    idx = returns.index.intersection(factor.index)\n",
    "    residuals = pd.DataFrame(index=idx, columns=returns.columns, dtype=float)\n",
    "    \n",
    "    for ticker in returns.columns:\n",
    "        data = pd.concat([returns[ticker], factor], axis=1).dropna()\n",
    "        data.columns = ['y', 'f']\n",
    "        \n",
    "        if len(data) < window:\n",
    "            continue\n",
    "        \n",
    "        # Rolling regression\n",
    "        X_with_const = sm.add_constant(data['f'])\n",
    "        rolling_model = RollingOLS(data['y'], X_with_const, window=window).fit()\n",
    "        \n",
    "        # Forward residual: realized(t+1) - predicted(t+1) using betas(t)\n",
    "        y_forward = data['y'].shift(-1)\n",
    "        f_forward = data['f'].shift(-1)\n",
    "        predicted = rolling_model.params['const'] + rolling_model.params['f'] * f_forward\n",
    "        \n",
    "        resid_forward = y_forward - predicted\n",
    "        residuals.loc[resid_forward.index, ticker] = resid_forward\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "spy_hedged_residuals = calculate_rolling_residuals_single_factor(\n",
    "    stock_returns, spy_returns, ROLLING_WINDOW_WEEKS\n",
    ")\n",
    "eval_42 = evaluate_forecasts(forecasts, spy_hedged_residuals)\n",
    "\n",
    "print(\"\\n4.2 - Forecast vs SPY-Hedged Residuals:\")\n",
    "print(f\"  Out-of-sample R²: {eval_42['oos_r2']:.4f}\")\n",
    "print(f\"  Correlation: {eval_42['correlation']:.4f}\")\n",
    "\n",
    "# Section 4.3 - Forecasts vs sector-hedged residuals\n",
    "def calculate_rolling_residuals_multi_factor(returns: pd.DataFrame, \n",
    "                                             factors: pd.DataFrame, \n",
    "                                             window: int) -> pd.DataFrame:\n",
    "    \"\"\"Calculate rolling residuals for each stock vs multiple factors\"\"\"\n",
    "    idx = returns.index.intersection(factors.index)\n",
    "    residuals = pd.DataFrame(index=idx, columns=returns.columns, dtype=float)\n",
    "    \n",
    "    for ticker in returns.columns:\n",
    "        data = pd.concat([returns[ticker].rename('y'), factors], axis=1).dropna()\n",
    "        \n",
    "        if len(data) < window:\n",
    "            continue\n",
    "        \n",
    "        y = data['y']\n",
    "        X = data.drop(columns=['y'])\n",
    "        X_with_const = sm.add_constant(X)\n",
    "        \n",
    "        # Rolling regression\n",
    "        rolling_model = RollingOLS(y, X_with_const, window=window).fit()\n",
    "        \n",
    "        # Forward residual\n",
    "        y_forward = y.shift(-1)\n",
    "        X_forward = X.shift(-1)\n",
    "        \n",
    "        predicted = rolling_model.params['const'].copy()\n",
    "        for col in X.columns:\n",
    "            predicted += rolling_model.params[col] * X_forward[col]\n",
    "        \n",
    "        resid_forward = y_forward - predicted\n",
    "        residuals.loc[resid_forward.index, ticker] = resid_forward\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "sector_hedged_residuals = calculate_rolling_residuals_multi_factor(\n",
    "    stock_returns, sector_returns, ROLLING_WINDOW_WEEKS\n",
    ")\n",
    "eval_43 = evaluate_forecasts(forecasts, sector_hedged_residuals)\n",
    "\n",
    "print(\"\\n4.3 - Forecast vs Sector-Hedged Residuals:\")\n",
    "print(f\"  Out-of-sample R²: {eval_43['oos_r2']:.4f}\")\n",
    "print(f\"  Correlation: {eval_43['correlation']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9b455",
   "metadata": {},
   "source": [
    "## Section 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d045ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.1 - LASSO Replication Weights (non-zero):\n",
      "XLY     0.4425\n",
      "XLC     0.4064\n",
      "XLI     0.3833\n",
      "XLF    -0.1781\n",
      "XLV     0.1495\n",
      "XLE    -0.0601\n",
      "XLU    -0.0543\n",
      "XLB    -0.0453\n",
      "XLP    -0.0312\n",
      "XLRE    0.0047\n",
      "dtype: float64\n",
      "\n",
      "5.2 - Arbitrage Analysis:\n",
      "  Replication R²: 0.7969\n",
      "  Tracking error (annualized): 0.1102\n",
      "  Mean forecast spread (annualized): 0.0025\n",
      "  Forecast spread volatility (ann): 0.0004\n",
      "\n",
      "  Largest absolute forecast spreads:\n",
      "date\n",
      "2020-03-20    0.000191\n",
      "2020-03-27    0.000175\n",
      "2020-04-03    0.000169\n",
      "2020-04-24    0.000150\n",
      "2020-04-17    0.000150\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get sector dividend yields for forecast consistency\n",
    "sector_div_yields = extract_field(sector_panel, 'EQY_DVD_YLD_IND')\n",
    "sector_total_returns = compute_total_returns(sector_prices, sector_div_yields)\n",
    "\n",
    "# Remove non-sector ETFs\n",
    "for exclude in ['SPY', 'SHV']:\n",
    "    if exclude in sector_total_returns.columns:\n",
    "        sector_total_returns = sector_total_returns.drop(columns=exclude)\n",
    "\n",
    "# Section 5.1 - LASSO Replication\n",
    "if LASSO_TARGET_TICKER not in sector_total_returns.columns:\n",
    "    print(f\"\\nWarning: Target {LASSO_TARGET_TICKER} not in sector data\")\n",
    "else:\n",
    "    target_returns = sector_total_returns[LASSO_TARGET_TICKER]\n",
    "    replication_universe = sector_total_returns.drop(columns=[LASSO_TARGET_TICKER])\n",
    "    \n",
    "    # Align and clean data\n",
    "    lasso_data = pd.concat([target_returns.rename('target'), replication_universe], axis=1).dropna()\n",
    "    y_lasso = lasso_data['target']\n",
    "    X_lasso = lasso_data.drop(columns=['target'])\n",
    "    \n",
    "    # LASSO with time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=LASSO_CV_SPLITS)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_lasso)\n",
    "    \n",
    "    lasso_model = LassoCV(cv=tscv, max_iter=10000, random_state=42)\n",
    "    lasso_model.fit(X_scaled, y_lasso)\n",
    "    \n",
    "    # Extract coefficients (unscaled)\n",
    "    coef_unscaled = lasso_model.coef_ / scaler.scale_\n",
    "    intercept_unscaled = lasso_model.intercept_ - np.sum((scaler.mean_ / scaler.scale_) * lasso_model.coef_)\n",
    "    \n",
    "    # Get non-zero weights\n",
    "    replication_weights = pd.Series(coef_unscaled, index=X_lasso.columns)\n",
    "    replication_weights = replication_weights[replication_weights != 0].sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\n5.1 - LASSO Replication Weights (non-zero):\")\n",
    "    print(replication_weights.round(4))\n",
    "    \n",
    "    # Build replication portfolio returns\n",
    "    rep_returns = (X_lasso @ replication_weights.reindex(X_lasso.columns).fillna(0)) + intercept_unscaled\n",
    "    \n",
    "    # Section 5.2 - Arbitrage Check via Forecast Consistency\n",
    "    # Compare dividend-yield forecasts\n",
    "    sector_yields_aligned = sector_div_yields.loc[lasso_data.index]\n",
    "    \n",
    "    # Forecast for target (weekly carry)\n",
    "    target_forecast = (sector_yields_aligned[LASSO_TARGET_TICKER] / 100.0) / WEEKS_PER_YEAR\n",
    "    \n",
    "    # Forecast for replication portfolio\n",
    "    if len(replication_weights) > 0:\n",
    "        rep_forecast = ((sector_yields_aligned[replication_weights.index] / 100.0) / WEEKS_PER_YEAR * \n",
    "                       replication_weights).sum(axis=1)\n",
    "    else:\n",
    "        rep_forecast = pd.Series(0, index=target_forecast.index)\n",
    "    \n",
    "    # Forecast spread (arbitrage indicator)\n",
    "    forecast_spread = target_forecast - rep_forecast\n",
    "    \n",
    "    # Tracking error\n",
    "    tracking_diff = y_lasso - rep_returns\n",
    "    tracking_error_ann = tracking_diff.std() * np.sqrt(WEEKS_PER_YEAR)\n",
    "    \n",
    "    print(\"\\n5.2 - Arbitrage Analysis:\")\n",
    "    print(f\"  Replication R²: {lasso_model.score(X_scaled, y_lasso):.4f}\")\n",
    "    print(f\"  Tracking error (annualized): {tracking_error_ann:.4f}\")\n",
    "    print(f\"  Mean forecast spread (annualized): {forecast_spread.mean() * WEEKS_PER_YEAR:.4f}\")\n",
    "    print(f\"  Forecast spread volatility (ann): {forecast_spread.std() * np.sqrt(WEEKS_PER_YEAR):.4f}\")\n",
    "    print(f\"\\n  Largest absolute forecast spreads:\")\n",
    "    print(forecast_spread.abs().nlargest(5).round(6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
